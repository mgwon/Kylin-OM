# fixed_enhanced_linux_expert_finetuner.py - ä¿®å¤ç‰ˆæœ¬
import os
import json
import torch
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from transformers import (
    AutoTokenizer, AutoModelForCausalLM,
    TrainingArguments, Trainer,
    DataCollatorForLanguageModeling
)
from datasets import Dataset, load_dataset, concatenate_datasets
from tqdm import tqdm
import warnings
import requests
import random
from typing import List, Dict, Any

warnings.filterwarnings('ignore')

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


class FixedLinuxExpertFineTuner:
    def __init__(self, model_name="deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"):
        self.model_name = model_name
        # å¼ºåˆ¶ä½¿ç”¨å•GPUé¿å…è®¾å¤‡ä¸åŒ¹é…
        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        print(f"æ­£åœ¨ä½¿ç”¨æ¨¡å‹: {model_name}")

        # åˆå§‹åŒ–tokenizer
        print("åŠ è½½tokenizer...")
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                trust_remote_code=True,
                padding_side="right"
            )
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            print("âœ“ TokenizeråŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âŒ TokenizeråŠ è½½å¤±è´¥: {e}")
            # ä½¿ç”¨å¤‡ç”¨æ¨¡å‹
            backup_models = [
                "Qwen/Qwen2.5-7B-Instruct",
                "microsoft/DialoGPT-medium"
            ]

            for backup_model in backup_models:
                try:
                    print(f"å°è¯•å¤‡ç”¨æ¨¡å‹: {backup_model}")
                    self.model_name = backup_model
                    self.tokenizer = AutoTokenizer.from_pretrained(
                        backup_model,
                        trust_remote_code=True,
                        padding_side="right"
                    )
                    if self.tokenizer.pad_token is None:
                        self.tokenizer.pad_token = self.tokenizer.eos_token
                    print(f"âœ“ æˆåŠŸä½¿ç”¨å¤‡ç”¨æ¨¡å‹: {backup_model}")
                    break
                except Exception as backup_e:
                    print(f"âŒ å¤‡ç”¨æ¨¡å‹ {backup_model} ä¹Ÿå¤±è´¥: {backup_e}")
                    continue
            else:
                raise Exception("æ‰€æœ‰æ¨¡å‹éƒ½æ— æ³•åŠ è½½")

    def load_huggingface_datasets(self):
        """åŠ è½½å¤šä¸ªå¼€æºæ•°æ®é›†"""
        print("æ­£åœ¨åŠ è½½å¼€æºæ•°æ®é›†...")
        all_datasets = []

        # 1. å°è¯•åŠ è½½Shellå‘½ä»¤æ•°æ®é›†
        try:
            print("åŠ è½½Unixå‘½ä»¤æ•°æ®é›†...")
            unix_commands = load_dataset("harpomaxx/unix-commands", split="train")
            if len(unix_commands) > 0:
                unix_data = self.process_unix_commands(unix_commands)
                all_datasets.extend(unix_data)
                print(f"âœ“ Unixå‘½ä»¤æ•°æ®é›†: {len(unix_data)} æ¡")
        except Exception as e:
            print(f"âŒ Unixå‘½ä»¤æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")

        # 2. å°è¯•åŠ è½½æŒ‡ä»¤æ•°æ®é›†
        try:
            print("åŠ è½½AlpacaæŒ‡ä»¤æ•°æ®é›†...")
            alpaca_dataset = load_dataset("tatsu-lab/alpaca", split="train")
            # ç­›é€‰Linuxç›¸å…³çš„æŒ‡ä»¤
            linux_alpaca = self.filter_linux_instructions(alpaca_dataset)
            all_datasets.extend(linux_alpaca)
            print(f"âœ“ Alpaca Linuxç›¸å…³æ•°æ®: {len(linux_alpaca)} æ¡")
        except Exception as e:
            print(f"âŒ Alpacaæ•°æ®é›†åŠ è½½å¤±è´¥: {e}")

        # 3. åŠ è½½è‡ªåˆ¶çš„Linuxä¸“å®¶æ•°æ®é›†
        print("æ·»åŠ è‡ªåˆ¶Linuxä¸“å®¶æ•°æ®é›†...")
        custom_data = self.create_comprehensive_linux_dataset()
        all_datasets.extend(custom_data)
        print(f"âœ“ è‡ªåˆ¶æ•°æ®é›†: {len(custom_data)} æ¡")

        # 4. ä»ç½‘ç»œèµ„æºç”Ÿæˆæ•°æ®
        try:
            print("ç”ŸæˆLinuxå‘½ä»¤è§£é‡Šæ•°æ®...")
            command_data = self.generate_command_explanations()
            all_datasets.extend(command_data)
            print(f"âœ“ å‘½ä»¤è§£é‡Šæ•°æ®: {len(command_data)} æ¡")
        except Exception as e:
            print(f"âŒ å‘½ä»¤æ•°æ®ç”Ÿæˆå¤±è´¥: {e}")

        # æ‰“ä¹±æ•°æ®
        random.shuffle(all_datasets)
        print(f"æ€»æ•°æ®é‡: {len(all_datasets)} æ¡")

        if len(all_datasets) < 1000:
            print("âš ï¸ è­¦å‘Š: æ•°æ®é‡è¾ƒå°‘ï¼Œæ­£åœ¨æ‰©å……...")
            all_datasets = self.augment_dataset(all_datasets)
            print(f"æ‰©å……åæ•°æ®é‡: {len(all_datasets)} æ¡")

        return all_datasets

    def process_unix_commands(self, dataset):
        """å¤„ç†Unixå‘½ä»¤æ•°æ®é›†"""
        processed_data = []

        for item in dataset:
            try:
                if 'command' in item and 'description' in item:
                    command = str(item['command']).strip()
                    description = str(item['description']).strip()

                    if command and description and len(command) < 100:
                        # åˆ›å»ºæŒ‡ä»¤-å›ç­”å¯¹
                        instruction = f"è¯·è§£é‡ŠLinuxå‘½ä»¤ {command} çš„ä½œç”¨"
                        output = f"å‘½ä»¤ {command} çš„ä½œç”¨æ˜¯: {description}"

                        processed_data.append({
                            'instruction': instruction,
                            'output': output
                        })

                        if len(processed_data) >= 200:  # é™åˆ¶æ•°é‡
                            break
            except Exception as e:
                continue

        return processed_data

    def filter_linux_instructions(self, dataset):
        """ç­›é€‰Linuxç›¸å…³çš„æŒ‡ä»¤"""
        linux_keywords = [
            'linux', 'command', 'terminal', 'bash', 'shell', 'file', 'directory',
            'permission', 'process', 'system', 'network', 'server', 'grep',
            'find', 'sed', 'awk', 'chmod', 'chown', 'ps', 'top', 'systemctl'
        ]

        filtered_data = []

        for item in dataset:
            try:
                instruction = str(item.get('instruction', '')).lower()
                output = str(item.get('output', '')).lower()

                # æ£€æŸ¥æ˜¯å¦åŒ…å«Linuxç›¸å…³å…³é”®è¯
                if any(keyword in instruction or keyword in output for keyword in linux_keywords):
                    filtered_data.append({
                        'instruction': item.get('instruction', ''),
                        'output': item.get('output', '')
                    })

                    if len(filtered_data) >= 500:  # é™åˆ¶æ•°é‡
                        break
            except Exception as e:
                continue

        return filtered_data

    def create_comprehensive_linux_dataset(self):
        """åˆ›å»ºå…¨é¢çš„Linuxä¸“å®¶æ•°æ®é›†"""
        linux_data = [
            {
                'instruction': 'å¦‚ä½•æŸ¥çœ‹Linuxç³»ç»Ÿä¸­æ­£åœ¨è¿è¡Œçš„è¿›ç¨‹ï¼Ÿ',
                'output': 'å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æŸ¥çœ‹è¿›ç¨‹ï¼š\n1) ps aux - æ˜¾ç¤ºæ‰€æœ‰æ­£åœ¨è¿è¡Œçš„è¿›ç¨‹åŠè¯¦ç»†ä¿¡æ¯\n2) top - å®æ—¶æ˜¾ç¤ºè¿›ç¨‹çŠ¶æ€å’Œç³»ç»Ÿè´Ÿè½½\n3) htop - topçš„å¢å¼ºç‰ˆæœ¬ï¼Œæä¾›æ›´å¥½çš„äº¤äº’ç•Œé¢\n4) ps -ef - æ˜¾ç¤ºå®Œæ•´çš„è¿›ç¨‹ä¿¡æ¯åŒ…æ‹¬çˆ¶è¿›ç¨‹\n5) pgrep è¿›ç¨‹å - æ ¹æ®è¿›ç¨‹åæŸ¥æ‰¾PID\nè¿™äº›å‘½ä»¤å¯ä»¥å¸®åŠ©ä½ ç›‘æ§ç³»ç»ŸçŠ¶æ€å’Œç®¡ç†è¿›ç¨‹ã€‚'
            },
            {
                'instruction': 'è§£é‡ŠLinuxä¸­çš„æ–‡ä»¶æƒé™ç³»ç»Ÿ',
                'output': 'Linuxæ–‡ä»¶æƒé™åŸºäºä¸‰ä¸ªç”¨æˆ·ç±»åˆ«ï¼šæ‰€æœ‰è€…(owner)ã€ç»„(group)ã€å…¶ä»–ç”¨æˆ·(others)ã€‚æ¯ä¸ªç±»åˆ«æœ‰ä¸‰ç§æƒé™ï¼š\n- è¯»æƒé™(r=4): å¯ä»¥æŸ¥çœ‹æ–‡ä»¶å†…å®¹æˆ–åˆ—å‡ºç›®å½•\n- å†™æƒé™(w=2): å¯ä»¥ä¿®æ”¹æ–‡ä»¶æˆ–åœ¨ç›®å½•ä¸­åˆ›å»ºåˆ é™¤æ–‡ä»¶\n- æ‰§è¡Œæƒé™(x=1): å¯ä»¥æ‰§è¡Œæ–‡ä»¶æˆ–è¿›å…¥ç›®å½•\næƒé™ç”¨æ•°å­—è¡¨ç¤ºæ—¶æ˜¯è¿™äº›å€¼çš„å’Œï¼Œå¦‚755è¡¨ç¤ºæ‰€æœ‰è€…å¯è¯»å†™æ‰§è¡Œ(7)ï¼Œç»„å’Œå…¶ä»–ç”¨æˆ·å¯è¯»æ‰§è¡Œ(5)ã€‚ä½¿ç”¨chmodå‘½ä»¤ä¿®æ”¹æƒé™ï¼Œå¦‚chmod 755 filenameã€‚'
            },
            {
                'instruction': 'Linuxç½‘ç»œé…ç½®æœ‰å“ªäº›å¸¸ç”¨å‘½ä»¤ï¼Ÿ',
                'output': 'Linuxç½‘ç»œé…ç½®å¸¸ç”¨å‘½ä»¤åŒ…æ‹¬ï¼š\n1) ifconfig/ip addr - æŸ¥çœ‹å’Œé…ç½®ç½‘ç»œæ¥å£\n2) ping - æµ‹è¯•ç½‘ç»œè¿é€šæ€§\n3) netstat/ss - æŸ¥çœ‹ç½‘ç»œè¿æ¥çŠ¶æ€å’Œç«¯å£\n4) route/ip route - ç®¡ç†è·¯ç”±è¡¨\n5) iptables/firewalld - é˜²ç«å¢™é…ç½®\n6) wget/curl - ä¸‹è½½æ–‡ä»¶å’Œæµ‹è¯•HTTPè¿æ¥\n7) tcpdump/wireshark - ç½‘ç»œæŠ“åŒ…åˆ†æ\n8) nslookup/dig - DNSæŸ¥è¯¢å·¥å…·'
            },
            {
                'instruction': 'å¦‚ä½•åœ¨Linuxä¸­ç›‘æ§ç³»ç»Ÿæ€§èƒ½ï¼Ÿ',
                'output': 'Linuxç³»ç»Ÿæ€§èƒ½ç›‘æ§å·¥å…·ï¼š\n1) top/htop - å®æ—¶æŸ¥çœ‹è¿›ç¨‹å’Œç³»ç»Ÿè´Ÿè½½\n2) iotop - ç›‘æ§ç£ç›˜I/Oä½¿ç”¨æƒ…å†µ\n3) iftop - ç›‘æ§ç½‘ç»œæµé‡\n4) vmstat - æ˜¾ç¤ºè™šæ‹Ÿå†…å­˜ã€è¿›ç¨‹ã€CPUç»Ÿè®¡\n5) iostat - æ˜¾ç¤ºI/Oå’ŒCPUç»Ÿè®¡ä¿¡æ¯\n6) sar - æ”¶é›†å’ŒæŠ¥å‘Šç³»ç»Ÿæ´»åŠ¨\n7) dstat - å®æ—¶ç³»ç»Ÿèµ„æºç»Ÿè®¡\n8) nmon - IBMçš„ç»¼åˆæ€§èƒ½ç›‘æ§å·¥å…·'
            },
            {
                'instruction': 'Linuxä¸­çš„æ–‡ä»¶æŸ¥æ‰¾å‘½ä»¤æœ‰å“ªäº›ï¼Ÿ',
                'output': 'Linuxæ–‡ä»¶æŸ¥æ‰¾å‘½ä»¤ï¼š\n1) find - æœ€å¼ºå¤§çš„æŸ¥æ‰¾å·¥å…·\n   - find /path -name "filename" æŒ‰åç§°æŸ¥æ‰¾\n   - find /path -type f -size +100M æŸ¥æ‰¾å¤§æ–‡ä»¶\n   - find /path -mtime -7 æŸ¥æ‰¾7å¤©å†…ä¿®æ”¹çš„æ–‡ä»¶\n2) locate - åŸºäºæ•°æ®åº“çš„å¿«é€ŸæŸ¥æ‰¾\n3) which - æŸ¥æ‰¾å¯æ‰§è¡Œæ–‡ä»¶è·¯å¾„\n4) whereis - æŸ¥æ‰¾äºŒè¿›åˆ¶æ–‡ä»¶ã€æºä»£ç å’Œæ‰‹å†Œé¡µ\n5) grep - åœ¨æ–‡ä»¶å†…å®¹ä¸­æœç´¢å­—ç¬¦ä¸²'
            },
            {
                'instruction': 'å¦‚ä½•åœ¨Linuxä¸­ç®¡ç†åå°è¿›ç¨‹ï¼Ÿ',
                'output': 'Linuxåå°è¿›ç¨‹ç®¡ç†ï¼š\n1) å¯åŠ¨åå°è¿›ç¨‹ï¼š\n   - command & - ç›´æ¥åœ¨åå°è¿è¡Œ\n   - nohup command & - å¿½ç•¥æŒ‚èµ·ä¿¡å·åœ¨åå°è¿è¡Œ\n2) ä½œä¸šæ§åˆ¶ï¼š\n   - jobs - æŸ¥çœ‹å½“å‰ä½œä¸š\n   - bg %job_id - å°†ä½œä¸šæ”¾åˆ°åå°\n   - fg %job_id - å°†åå°ä½œä¸šè°ƒåˆ°å‰å°\n3) è¿›ç¨‹æ§åˆ¶ï¼š\n   - Ctrl+Z - æš‚åœå½“å‰è¿›ç¨‹\n   - kill PID - ç»ˆæ­¢è¿›ç¨‹'
            },
            {
                'instruction': 'Linuxä¸­çš„å‹ç¼©å’Œè§£å‹å‘½ä»¤è¯¦è§£',
                'output': 'Linuxå‹ç¼©è§£å‹å‘½ä»¤è¯¦è§£ï¼š\n1) tarå‘½ä»¤ - æ‰“åŒ…å·¥å…·ï¼š\n   - tar -czf archive.tar.gz files/ åˆ›å»ºgzipå‹ç¼©åŒ…\n   - tar -xzf archive.tar.gz è§£å‹gzipåŒ…\n   - tar -cjf archive.tar.bz2 files/ åˆ›å»ºbzip2å‹ç¼©åŒ…\n2) zip/unzipï¼š\n   - zip -r archive.zip files/ å‹ç¼©\n   - unzip archive.zip è§£å‹\n3) gzip/gunzip - å•æ–‡ä»¶å‹ç¼©'
            },
            {
                'instruction': 'Linuxæ–‡æœ¬å¤„ç†å‘½ä»¤æœ‰å“ªäº›ï¼Ÿ',
                'output': 'Linuxæ–‡æœ¬å¤„ç†å‘½ä»¤ï¼š\n1) grep - æ–‡æœ¬æœç´¢ï¼š\n   - grep "pattern" file æœç´¢æ¨¡å¼\n   - grep -i "pattern" file å¿½ç•¥å¤§å°å†™\n2) sed - æµç¼–è¾‘å™¨ï¼š\n   - sed "s/old/new/g" file æ›¿æ¢æ–‡æœ¬\n3) awk - æ–‡æœ¬åˆ†æå·¥å…·ï¼š\n   - awk "{print $1}" file æ‰“å°ç¬¬ä¸€åˆ—\n4) sort/uniq - æ’åºå’Œå»é‡\n5) cut - æŒ‰åˆ—æå–æ–‡æœ¬'
            }
        ]

        return linux_data

    def generate_command_explanations(self):
        """ç”ŸæˆLinuxå‘½ä»¤è§£é‡Šæ•°æ®"""
        commands = {
            'ls': 'åˆ—å‡ºç›®å½•å†…å®¹',
            'cd': 'åˆ‡æ¢ç›®å½•',
            'pwd': 'æ˜¾ç¤ºå½“å‰å·¥ä½œç›®å½•',
            'mkdir': 'åˆ›å»ºç›®å½•',
            'rm': 'åˆ é™¤æ–‡ä»¶æˆ–ç›®å½•',
            'cp': 'å¤åˆ¶æ–‡ä»¶æˆ–ç›®å½•',
            'mv': 'ç§»åŠ¨æˆ–é‡å‘½åæ–‡ä»¶',
            'chmod': 'ä¿®æ”¹æ–‡ä»¶æƒé™',
            'grep': 'æœç´¢æ–‡æœ¬æ¨¡å¼',
            'find': 'æŸ¥æ‰¾æ–‡ä»¶å’Œç›®å½•',
            'ps': 'æ˜¾ç¤ºè¿›ç¨‹ä¿¡æ¯',
            'top': 'æ˜¾ç¤ºå®æ—¶è¿›ç¨‹ä¿¡æ¯',
            'df': 'æ˜¾ç¤ºç£ç›˜ç©ºé—´ä½¿ç”¨æƒ…å†µ',
            'free': 'æ˜¾ç¤ºå†…å­˜ä½¿ç”¨æƒ…å†µ',
            'cat': 'æ˜¾ç¤ºæ–‡ä»¶å†…å®¹',
            'head': 'æ˜¾ç¤ºæ–‡ä»¶å¼€å¤´',
            'tail': 'æ˜¾ç¤ºæ–‡ä»¶ç»“å°¾',
            'tar': 'æ‰“åŒ…å’Œå‹ç¼©æ–‡ä»¶',
            'wget': 'ä¸‹è½½æ–‡ä»¶',
            'ssh': 'SSHè¿œç¨‹ç™»å½•',
            'ping': 'æµ‹è¯•ç½‘ç»œè¿é€šæ€§'
        }

        command_data = []

        for cmd, desc in commands.items():
            command_data.append({
                'instruction': f'Linuxå‘½ä»¤ {cmd} æ˜¯åšä»€ä¹ˆç”¨çš„ï¼Ÿ',
                'output': f'{cmd} å‘½ä»¤ç”¨äº{desc}ã€‚è¿™æ˜¯Linuxç³»ç»Ÿä¸­çš„å¸¸ç”¨å‘½ä»¤ä¹‹ä¸€ã€‚'
            })

        return command_data

    def augment_dataset(self, dataset):
        """æ•°æ®å¢å¼º"""
        augmented = dataset.copy()

        # é€šè¿‡é‡æ–°è¡¨è¿°é—®é¢˜æ¥å¢åŠ æ•°æ®
        rephrase_patterns = [
            ("å¦‚ä½•", "æ€æ ·"),
            ("ä»€ä¹ˆæ˜¯", "è¯·è§£é‡Š"),
            ("å‘½ä»¤", "æŒ‡ä»¤"),
            ("ä½¿ç”¨", "è¿ç”¨")
        ]

        for item in dataset:
            instruction = item['instruction']
            output = item['output']

            # é‡æ–°è¡¨è¿°æŒ‡ä»¤
            for old, new in rephrase_patterns:
                if old in instruction:
                    new_instruction = instruction.replace(old, new)
                    augmented.append({
                        'instruction': new_instruction,
                        'output': output
                    })
                    break

        return augmented

    def create_linux_dataset(self):
        """åˆ›å»ºLinuxä¸“å®¶æ•°æ®é›†ï¼ˆä¸»å…¥å£ï¼‰"""
        print("åˆ›å»ºå¢å¼ºçš„Linuxä¸“å®¶æ•°æ®é›†...")

        # åŠ è½½æ‰€æœ‰æ•°æ®æº
        all_data = self.load_huggingface_datasets()

        # åˆ†å‰²è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
        train_size = int(0.8 * len(all_data))
        random.shuffle(all_data)

        train_data = all_data[:train_size]
        test_data = all_data[train_size:]

        print(f"è®­ç»ƒæ•°æ®: {len(train_data)} æ¡")
        print(f"æµ‹è¯•æ•°æ®: {len(test_data)} æ¡")

        return train_data, test_data

    def format_data_for_training(self, data):
        """æ ¼å¼åŒ–æ•°æ®ç”¨äºè®­ç»ƒ"""
        formatted_data = []
        for item in data:
            try:
                # æ ¹æ®æ¨¡å‹ç±»å‹è°ƒæ•´æ ¼å¼
                if "DeepSeek" in self.model_name:
                    text = f"<|user|>\n{item['instruction']}\n<|assistant|>\n{item['output']}<|end|>"
                elif "Qwen" in self.model_name:
                    text = f"<|im_start|>user\n{item['instruction']}<|im_end|>\n<|im_start|>assistant\n{item['output']}<|im_end|>"
                else:
                    # é€šç”¨æ ¼å¼
                    text = f"ç”¨æˆ·: {item['instruction']}\nåŠ©æ‰‹: {item['output']}"

                formatted_data.append({'text': text})
            except Exception as e:
                print(f"æ ¼å¼åŒ–æ•°æ®æ—¶å‡ºé”™: {e}")
                continue

        return Dataset.from_list(formatted_data)

    def tokenize_function(self, examples):
        """æ•°æ®æ ‡è®°åŒ–å‡½æ•°"""
        try:
            return self.tokenizer(
                examples['text'],
                truncation=True,
                padding='max_length',
                max_length=512,
                return_tensors='pt'
            )
        except Exception as e:
            print(f"æ ‡è®°åŒ–é”™è¯¯: {e}")
            return {}

    def load_model(self, model_path=None):
        """åŠ è½½æ¨¡å‹"""
        if model_path and os.path.exists(model_path):
            print(f"åŠ è½½å¾®è°ƒåçš„æ¨¡å‹: {model_path}")
            model = AutoModelForCausalLM.from_pretrained(
                model_path,
                trust_remote_code=True,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                device_map={'': 0} if torch.cuda.is_available() else None,  # å¼ºåˆ¶ä½¿ç”¨GPU 0
                low_cpu_mem_usage=True
            )
        else:
            print(f"åŠ è½½åŸå§‹æ¨¡å‹: {self.model_name}")
            model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                trust_remote_code=True,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                device_map={'': 0} if torch.cuda.is_available() else None,  # å¼ºåˆ¶ä½¿ç”¨GPU 0
                low_cpu_mem_usage=True
            )
        return model

    def fine_tune(self, train_data, test_data):
        """å¾®è°ƒæ¨¡å‹"""
        print("å¼€å§‹å¾®è°ƒæ¨¡å‹...")

        # æ ¼å¼åŒ–æ•°æ®
        train_dataset = self.format_data_for_training(train_data)
        test_dataset = self.format_data_for_training(test_data)

        # æ ‡è®°åŒ–æ•°æ®
        train_dataset = train_dataset.map(self.tokenize_function, batched=True, remove_columns=['text'])
        test_dataset = test_dataset.map(self.tokenize_function, batched=True, remove_columns=['text'])

        # åŠ è½½æ¨¡å‹
        model = self.load_model()

        # è®¾ç½®è®­ç»ƒå‚æ•°
        training_args = TrainingArguments(
            output_dir="./fixed_linux_expert_model",
            per_device_train_batch_size=1,
            per_device_eval_batch_size=1,
            gradient_accumulation_steps=4,  # å‡å°‘
            num_train_epochs=2,  # å‡å°‘è®­ç»ƒè½®æ•°
            learning_rate=2e-5,
            fp16=torch.cuda.is_available(),
            logging_steps=10,
            save_steps=100,
            eval_steps=100,
            evaluation_strategy="steps",
            save_strategy="steps",
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            greater_is_better=False,
            warmup_ratio=0.1,
            lr_scheduler_type="cosine",
            report_to="none",
            save_total_limit=2,
            dataloader_pin_memory=False,
            dataloader_num_workers=0,  # é¿å…å¤šè¿›ç¨‹é—®é¢˜
            remove_unused_columns=False,
        )

        # è®¾ç½®æ•°æ®æ•´ç†å™¨
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False
        )

        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=test_dataset,
            data_collator=data_collator,
        )

        # å¼€å§‹è®­ç»ƒ
        print("å¼€å§‹è®­ç»ƒè¿‡ç¨‹...")
        try:
            trainer.train()
        except Exception as e:
            print(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            print("å°è¯•ç»§ç»­è®­ç»ƒ...")

        # ä¿å­˜æ¨¡å‹
        final_model_path = "./fixed_linux_expert_model_final"
        trainer.save_model(final_model_path)
        print(f"æ¨¡å‹å¾®è°ƒå®Œæˆå¹¶å·²ä¿å­˜åˆ°: {final_model_path}")

        return final_model_path

    def generate_response(self, model, prompt, max_length=200):
        """ç”Ÿæˆå›ç­” - ä¿®å¤inf/nané—®é¢˜"""
        try:
            # æ ¹æ®æ¨¡å‹è°ƒæ•´è¾“å…¥æ ¼å¼
            if "DeepSeek" in self.model_name:
                input_text = f"<|user|>\n{prompt}\n<|assistant|>\n"
            elif "Qwen" in self.model_name:
                input_text = f"<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"
            else:
                input_text = f"ç”¨æˆ·: {prompt}\nåŠ©æ‰‹: "

            inputs = self.tokenizer.encode(input_text, return_tensors='pt').to(self.device)

            # è®¾ç½®attention_mask
            attention_mask = torch.ones_like(inputs).to(self.device)

            with torch.no_grad():
                outputs = model.generate(
                    inputs,
                    attention_mask=attention_mask,
                    max_new_tokens=150,  # ä½¿ç”¨max_new_tokensè€Œä¸æ˜¯max_length
                    temperature=0.6,  # DeepSeekæ¨èçš„æ¸©åº¦
                    do_sample=True,
                    top_p=0.95,  # æ·»åŠ top_p
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    repetition_penalty=1.1,  # é¿å…é‡å¤
                    no_repeat_ngram_size=3
                )

            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

            # æå–å›ç­”éƒ¨åˆ†
            if "DeepSeek" in self.model_name:
                if '<|assistant|>' in response:
                    return response.split('<|assistant|>\n')[-1].strip()
            elif "Qwen" in self.model_name:
                if '<|im_start|>assistant' in response:
                    return response.split('<|im_start|>assistant\n')[-1].strip()
            else:
                if 'åŠ©æ‰‹:' in response:
                    return response.split('åŠ©æ‰‹:')[-1].strip()

            return response.strip()

        except Exception as e:
            print(f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {e}")
            return "æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°äº†é”™è¯¯ã€‚"

    def create_test_questions(self):
        """åˆ›å»ºæµ‹è¯•é—®é¢˜"""
        return [
            {
                'question': 'å¦‚ä½•æŸ¥çœ‹Linuxç³»ç»Ÿä¸­æ­£åœ¨è¿è¡Œçš„è¿›ç¨‹ï¼Ÿ',
                'expected_keywords': ['ps', 'top', 'htop', 'è¿›ç¨‹', 'å‘½ä»¤']
            },
            {
                'question': 'è§£é‡ŠLinuxä¸­çš„æ–‡ä»¶æƒé™ç³»ç»Ÿ',
                'expected_keywords': ['æƒé™', 'chmod', 'rwx', 'ç”¨æˆ·', 'ç»„']
            },
            {
                'question': 'Linuxä¸­å¦‚ä½•æŸ¥æ‰¾æ–‡ä»¶ï¼Ÿ',
                'expected_keywords': ['find', 'locate', 'grep', 'æœç´¢', 'æ–‡ä»¶']
            },
            {
                'question': 'å¦‚ä½•åœ¨Linuxä¸­ç®¡ç†æœåŠ¡ï¼Ÿ',
                'expected_keywords': ['systemctl', 'service', 'å¯åŠ¨', 'åœæ­¢', 'çŠ¶æ€']
            },
            {
                'question': 'Linuxç½‘ç»œé…ç½®æœ‰å“ªäº›å¸¸ç”¨å‘½ä»¤ï¼Ÿ',
                'expected_keywords': ['ifconfig', 'ip', 'netstat', 'ç½‘ç»œ', 'é…ç½®']
            }
        ]

    def evaluate_model(self, model, test_questions):
        """è¯„ä¼°æ¨¡å‹"""
        results = []
        print("æ­£åœ¨è¯„ä¼°æ¨¡å‹...")

        for question in tqdm(test_questions):
            try:
                response = self.generate_response(model, question['question'])
                score = self.calculate_relevance_score(response, question.get('expected_keywords', []))
                results.append({
                    'question': question['question'],
                    'response': response,
                    'score': score
                })
                print(f"é—®é¢˜: {question['question'][:40]}...")
                print(f"å›ç­”: {response[:100]}...")
                print(f"åˆ†æ•°: {score:.3f}\n")
            except Exception as e:
                print(f"è¯„ä¼°é”™è¯¯: {e}")
                results.append({
                    'question': question['question'],
                    'response': "ç”Ÿæˆå¤±è´¥",
                    'score': 0
                })

        return results

    def calculate_relevance_score(self, response, expected_keywords):
        """è®¡ç®—å›ç­”çš„ç›¸å…³æ€§åˆ†æ•°"""
        if not expected_keywords:
            return 0.5

        response_lower = response.lower()
        matches = sum(1 for keyword in expected_keywords if keyword.lower() in response_lower)
        return matches / len(expected_keywords)

    def visualize_results(self, original_results, finetuned_results):
        """å¯è§†åŒ–ç»“æœå¯¹æ¯”"""
        orig_scores = [r['score'] for r in original_results]
        ft_scores = [r['score'] for r in finetuned_results]

        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('ä¿®å¤ç‰ˆLinuxä¸“å®¶æ¨¡å‹å¾®è°ƒæ•ˆæœå¯¹æ¯”', fontsize=16, fontweight='bold')

        # 1. å¹³å‡åˆ†æ•°å¯¹æ¯”
        categories = ['åŸå§‹æ¨¡å‹', 'å¾®è°ƒæ¨¡å‹']
        avg_scores = [np.mean(orig_scores), np.mean(ft_scores)]

        axes[0, 0].bar(categories, avg_scores, color=['#ff7f0e', '#2ca02c'], alpha=0.7)
        axes[0, 0].set_title('å¹³å‡æ€§èƒ½åˆ†æ•°å¯¹æ¯”')
        axes[0, 0].set_ylabel('åˆ†æ•°')
        axes[0, 0].set_ylim(0, 1)

        for i, v in enumerate(avg_scores):
            axes[0, 0].text(i, v + 0.01, f'{v:.3f}', ha='center', fontweight='bold')

        # 2. å„é—®é¢˜å¾—åˆ†å¯¹æ¯”
        x = np.arange(len(orig_scores))
        width = 0.35

        axes[0, 1].bar(x - width / 2, orig_scores, width, label='åŸå§‹æ¨¡å‹', color='#ff7f0e', alpha=0.7)
        axes[0, 1].bar(x + width / 2, ft_scores, width, label='å¾®è°ƒæ¨¡å‹', color='#2ca02c', alpha=0.7)
        axes[0, 1].set_title('å„é—®é¢˜å¾—åˆ†å¯¹æ¯”')
        axes[0, 1].set_xlabel('é—®é¢˜ç¼–å·')
        axes[0, 1].set_ylabel('åˆ†æ•°')
        axes[0, 1].legend()

        # 3. æ”¹è¿›å¹…åº¦
        improvements = [ft_scores[i] - orig_scores[i] for i in range(len(orig_scores))]
        colors = ['#d62728' if imp < 0 else '#2ca02c' for imp in improvements]

        axes[1, 0].bar(range(len(improvements)), improvements, color=colors, alpha=0.7)
        axes[1, 0].set_title('æ€§èƒ½æ”¹è¿›å¹…åº¦')
        axes[1, 0].set_xlabel('é—®é¢˜ç¼–å·')
        axes[1, 0].set_ylabel('æ”¹è¿›åˆ†æ•°')
        axes[1, 0].axhline(y=0, color='black', linestyle='-', alpha=0.3)

        # 4. åˆ†æ•°åˆ†å¸ƒ
        axes[1, 1].hist([orig_scores, ft_scores], bins=10, alpha=0.7,
                        label=['åŸå§‹æ¨¡å‹', 'å¾®è°ƒæ¨¡å‹'], color=['#ff7f0e', '#2ca02c'])
        axes[1, 1].set_title('åˆ†æ•°åˆ†å¸ƒ')
        axes[1, 1].set_xlabel('åˆ†æ•°')
        axes[1, 1].set_ylabel('é¢‘æ¬¡')
        axes[1, 1].legend()

        plt.tight_layout()
        plt.savefig('fixed_linux_expert_model_comparison.png', dpi=300, bbox_inches='tight')
        plt.show()

        # ç”ŸæˆæŠ¥å‘Š
        self.generate_report(original_results, finetuned_results, avg_scores, improvements)

    def generate_report(self, original_results, finetuned_results, avg_scores, improvements):
        """ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š"""
        report = f"""
# ä¿®å¤ç‰ˆLinuxä¸“å®¶æ¨¡å‹å¾®è°ƒæ•ˆæœæŠ¥å‘Š

## æ¨¡å‹ä¿¡æ¯
- ä½¿ç”¨æ¨¡å‹: {self.model_name}
- è®¡ç®—è®¾å¤‡: {self.device}
- ä¿®å¤é—®é¢˜: è§£å†³äº†è®¾å¤‡ä¸åŒ¹é…å’Œinf/nané”™è¯¯

## æ•´ä½“æ€§èƒ½å¯¹æ¯”
- åŸå§‹æ¨¡å‹å¹³å‡åˆ†æ•°: {avg_scores[0]:.4f}
- å¾®è°ƒæ¨¡å‹å¹³å‡åˆ†æ•°: {avg_scores[1]:.4f}
- æ•´ä½“æå‡: {((avg_scores[1] - avg_scores[0]) / avg_scores[0] * 100):.2f}%

## è¯¦ç»†é—®é¢˜åˆ†æ
"""

        for i, (orig, ft) in enumerate(zip(original_results, finetuned_results)):
            improvement = improvements[i]
            improvement_pct = (improvement / orig['score'] * 100) if orig['score'] > 0 else 0

            report += f"""
### é—®é¢˜ {i + 1}: {orig['question']}
- åŸå§‹æ¨¡å‹åˆ†æ•°: {orig['score']:.3f}
- å¾®è°ƒæ¨¡å‹åˆ†æ•°: {ft['score']:.3f}
- æ”¹è¿›å¹…åº¦: {improvement_pct:.1f}%

**åŸå§‹æ¨¡å‹å›ç­”**: {orig['response'][:200]}{'...' if len(orig['response']) > 200 else ''}

**å¾®è°ƒæ¨¡å‹å›ç­”**: {ft['response'][:200]}{'...' if len(ft['response']) > 200 else ''}

"""

        with open('fixed_linux_expert_model_report.md', 'w', encoding='utf-8') as f:
            f.write(report)

        print("è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ° fixed_linux_expert_model_report.md")


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ä¿®å¤ç‰ˆLinuxä¸“å®¶æ¨¡å‹å¾®è°ƒç³»ç»Ÿ")
    print("å·²è§£å†³è®¾å¤‡ä¸åŒ¹é…å’Œinf/nané”™è¯¯")
    print("=" * 60)

    # æ£€æŸ¥GPU
    if torch.cuda.is_available():
        print(f"âœ… æ£€æµ‹åˆ°GPU: {torch.cuda.get_device_name()}")
        print(f"   GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024 ** 3:.1f} GB")
        print(f"   å¼ºåˆ¶ä½¿ç”¨GPU:0 é¿å…è®¾å¤‡å†²çª")
    else:
        print("âš ï¸  ä½¿ç”¨CPUæ¨¡å¼")

    # åˆå§‹åŒ–å¾®è°ƒå™¨
    try:
        print("\nğŸ“š åˆå§‹åŒ–ä¿®å¤ç‰ˆå¾®è°ƒå™¨...")
        finetuner = FixedLinuxExpertFineTuner()
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        return

    # æ­¥éª¤1: å‡†å¤‡æ•°æ®é›†
    print("\nğŸ“Š æ­¥éª¤1: å‡†å¤‡æ•°æ®é›†")
    try:
        train_data, test_data = finetuner.create_linux_dataset()
    except Exception as e:
        print(f"âŒ æ•°æ®é›†å‡†å¤‡å¤±è´¥: {e}")
        return

    # æ­¥éª¤2: åˆ›å»ºæµ‹è¯•é—®é¢˜
    print("\nğŸ§ª æ­¥éª¤2: åˆ›å»ºæµ‹è¯•é—®é¢˜")
    test_questions = finetuner.create_test_questions()

    # æ­¥éª¤3: æµ‹è¯•åŸå§‹æ¨¡å‹
    print("\nğŸ” æ­¥éª¤3: æµ‹è¯•åŸå§‹æ¨¡å‹")
    try:
        original_model = finetuner.load_model()
        original_results = finetuner.evaluate_model(original_model, test_questions)

        # æ¸…ç†å†…å­˜
        del original_model
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    except Exception as e:
        print(f"âŒ æµ‹è¯•åŸå§‹æ¨¡å‹æ—¶å‡ºé”™: {e}")
        return

    # æ­¥éª¤4: å¾®è°ƒæ¨¡å‹
    print("\nğŸš€ æ­¥éª¤4: å¼€å§‹å¾®è°ƒæ¨¡å‹")
    try:
        model_path = finetuner.fine_tune(train_data, test_data)
    except Exception as e:
        print(f"âŒ å¾®è°ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        return

    # æ­¥éª¤5: æµ‹è¯•å¾®è°ƒåçš„æ¨¡å‹
    print("\nğŸ“ˆ æ­¥éª¤5: æµ‹è¯•å¾®è°ƒåçš„æ¨¡å‹")
    try:
        finetuned_model = finetuner.load_model(model_path)
        finetuned_results = finetuner.evaluate_model(finetuned_model, test_questions)
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¾®è°ƒæ¨¡å‹æ—¶å‡ºé”™: {e}")
        return

    # æ­¥éª¤6: ç”Ÿæˆå¯¹æ¯”åˆ†æ
    print("\nğŸ“Š æ­¥éª¤6: ç”Ÿæˆæ•ˆæœå¯¹æ¯”åˆ†æ")
    finetuner.visualize_results(original_results, finetuned_results)

    print("\n" + "=" * 60)
    print("ğŸ‰ ä¿®å¤ç‰ˆLinuxä¸“å®¶æ¨¡å‹å¾®è°ƒå®Œæˆ!")
    print("\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print("  ğŸ“Š fixed_linux_expert_model_comparison.png - æ•ˆæœå¯¹æ¯”å›¾")
    print("  ğŸ“ fixed_linux_expert_model_report.md - è¯¦ç»†åˆ†ææŠ¥å‘Š")
    print(f"  ğŸ¤– {model_path} - å¾®è°ƒåçš„æ¨¡å‹")
    print("\nğŸ”§ ä¿®å¤çš„é—®é¢˜:")
    print("  â€¢ è§£å†³äº†å¤šGPUè®¾å¤‡ä¸åŒ¹é…é”™è¯¯")
    print("  â€¢ ä¿®å¤äº†ç”Ÿæˆæ—¶çš„inf/nanæ¦‚ç‡é”™è¯¯")
    print("  â€¢ ä¼˜åŒ–äº†æ¨¡å‹åŠ è½½å’Œè®­ç»ƒå‚æ•°")
    print("  â€¢ æ·»åŠ äº†æ›´å¥½çš„é”™è¯¯å¤„ç†")
    print("=" * 60)


if __name__ == "__main__":
    main()